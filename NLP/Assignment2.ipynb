{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfxfjYQ8HD4P"
      },
      "source": [
        "**Name: Debangshu Bhattacharya**\n",
        "\n",
        "**Roll: MDS201910**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6X2RNYRJxsg",
        "outputId": "c62422ba-78b5-4bd5-8cbe-928eb8ae2529"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcHRgYxQ4s4k",
        "outputId": "955ea3fe-5515-4d13-8dd6-e2318a16b948"
      },
      "source": [
        "#importing necessary library and packages\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import time\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA0zuZ-RJj3p"
      },
      "source": [
        "#reading preprocessed_corpus\n",
        "wd = 'My Drive/NLP/'\n",
        "with open(os.path.join(wd, 'preprocessed_corpus.txt'), 'r') as f:\n",
        "  corpus = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsn_6VfsKOOP",
        "outputId": "a2b1c2dc-24fb-4ad1-90d2-6cdb23c9335f"
      },
      "source": [
        "#transforming the preprocessed corpus as a list of sentences. In my preprocessing I was separating each sentence by the start symbol '$$$$' and end symbol '$$$$' respectively.\n",
        "#Also documents were separaated by '\\n'. Here, I am handling that and transforming the corpus into a list of sentences.\n",
        "sentences = []\n",
        "docs = corpus.split('\\n')\n",
        "for doc in tqdm(docs):\n",
        "  sentences_doc = doc.split('$$$$$$$$')\n",
        "  t = sentences_doc[0]\n",
        "  sentences_doc[0] = t[4:]\n",
        "\n",
        "  t = sentences_doc[-1]\n",
        "  sentences_doc[-1] = t[:-4]\n",
        "  \n",
        "  for sent in sentences_doc:\n",
        "    sentences.append(sent)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 54941/54941 [00:04<00:00, 12096.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raDCgm5Y5u6a",
        "outputId": "3ca87fdd-6d97-4a38-98d4-3a61b1d95d0e"
      },
      "source": [
        "# randomly selecting a subset of the corpus to work with. If I am performing the experiment on the whole corpus, my RAM is crashing.\n",
        "# So, after experimenting on the value of the corpus to be chosen \n",
        "random.seed(42)\n",
        "N = len(sentences)\n",
        "subset_size = 700000\n",
        "sentence_idxs = random.sample(range(N), subset_size)\n",
        "sentence_subset = [sentences[i] for i in sentence_idxs]\n",
        "\n",
        "len(sentence_subset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hoc0aFB1P-Gs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_sentences, test_sentences = train_test_split(sentence_subset, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHjg06xoROFH",
        "outputId": "f8620166-233e-4834-fbf9-81f4e1e8f76d"
      },
      "source": [
        "len(train_sentences), len(test_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(630000, 70000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOyzpIf1uc9"
      },
      "source": [
        "del corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKrGPw_HDRO-"
      },
      "source": [
        "del sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZxqWuOOASD5"
      },
      "source": [
        "## Trigram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEE86Jfej6AO"
      },
      "source": [
        "def build_trigram_model(sentences):\n",
        "  #function to build a trigram model with a list of sentences as input\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  #iterating over sentences\n",
        "  for sentence in tqdm(sentences):\n",
        "    #tokenizing the sentence into a list of words\n",
        "    tokens = word_tokenize(sentence)\n",
        "    words = [word for word in tokens if word.isalpha()]\n",
        "    #finding the trigrams for the words for the sentence\n",
        "    trigrams = list(ngrams(words,3, pad_left = True, pad_right = True, left_pad_symbol = '<s>', right_pad_symbol = '</s>'))\n",
        "    #updating frequency of each parameter\n",
        "    for w1,w2,w3 in trigrams:\n",
        "      model[(w1,w2)][w3]+=1\n",
        "\n",
        "  #updating probability of each parameter    \n",
        "  for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYKV2dP9wDmi"
      },
      "source": [
        "#generate sentence based on trigram language model\n",
        "def generate_sentence_from_trigram_model(model, first_word_argmax = False, deterministic = False):\n",
        "  '''function to generate sentence with trigram model given as input. The parameter \"first_word_argmax\" is used to determine if the first word to be chosen is going to be the word which has maximum occurence.\n",
        "  If first_word_argmax = True, then the model chooses the word which has the highest single occurence, else it chooses the first word randomly based on predicted probability of words.\n",
        "  The parameter \"deterministic\" is used to determine if the language to be generated is deterministic or not.\n",
        "  \n",
        "  For the rest of the words to be generated, if the parameter \"deterministic\" is True, then we generate word (w3) based on the \n",
        "  maximum probability given the previous two words(w1,w2). If the parameter \"deterministic\" is False, then we generate a word randomly based on the probablity distribution of w3|w1,w2.   \n",
        "  '''\n",
        "  w1 = '<s>'\n",
        "  w2 = '<s>'\n",
        "\n",
        "  sent = ''\n",
        "  words = list(model[(w1,w2)].keys())\n",
        "  probs = list(model[(w1,w2)].values())\n",
        "\n",
        "  if first_word_argmax == True:\n",
        "    word = words[np.argmax(probs)]\n",
        "  else:\n",
        "    word = np.random.choice(words, p  = probs)\n",
        "  \n",
        "  first_word = word\n",
        "\n",
        "  sent = sent + first_word\n",
        "  sentence_finish = False\n",
        "  while not sentence_finish:\n",
        "    w1 = w2\n",
        "    w2 = word\n",
        "\n",
        "    words = list(model[(w1,w2)].keys())\n",
        "    probs = list(model[(w1,w2)].values())\n",
        "\n",
        "    if not deterministic:\n",
        "      word = np.random.choice(words, p = probs)\n",
        "    else:\n",
        "      word = words[np.argmax(probs)]\n",
        "\n",
        "    #print (word)\n",
        "    if word=='</s>':\n",
        "      sentence_finish = True\n",
        "      break\n",
        "    sent = sent + ' ' + str(word)\n",
        "\n",
        "    \n",
        "  return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThweoX8i17Km"
      },
      "source": [
        "**1) Language Model of trigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XlkulhP-Lo_",
        "outputId": "1dc84f94-16ba-4d67-9dc2-5d4008f591e2"
      },
      "source": [
        "print (\"Building trigram language model\")\n",
        "\n",
        "start_clock = time.time()\n",
        "model = build_trigram_model(train_sentences)\n",
        "end_clock = time.time()\n",
        "print (\"Model built\")\n",
        "\n",
        "print (\"Time taken = %.3f minutes\" %((end_clock-start_clock)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/630000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building trigram language model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 630000/630000 [02:52<00:00, 3661.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model built\n",
            "Time taken = 2.978 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6Oq_ERt2J_-"
      },
      "source": [
        "**2) Generating a sentence using trigram model**\n",
        "\n",
        "Generating a sentence based on model parameters, i.e, choosing word w3 given w1 and w2. \n",
        "\n",
        "Based on the model's predicted probability of word w3 given w1 and w2, i.e,  P(w3|w1,w2), I am going to show how the model generates sentences over three variants of probability model. \n",
        "\n",
        "#### a) First word is the word with highest occurence in the corpus and rest of the words generated are one with the highest P(w3|w1,w2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nQxqR-YyFnH",
        "outputId": "27e03143-a1b9-4cba-8888-26cb5b6619c6"
      },
      "source": [
        "print (\"Generating sentence\")\n",
        "start_clock = time.time()\n",
        "generated_sent = generate_sentence_from_trigram_model(model, first_word_argmax = True, deterministic=True)\n",
        "end_clock = time.time()\n",
        "print (\"Generated sentence: %s\" %generated_sent)\n",
        "print (\"Time taken = %.3f seconds\" %((end_clock-start_clock)/60))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentence\n",
            "Generated sentence: the copyright holder for this preprint this version posted may\n",
            "Time taken = 0.000 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa6tyxtUMCrD"
      },
      "source": [
        "#### b) Choosing first word randomly based on the probability distribution of P(first_word| start_token, start_token) and for the rest of the words selecting the word w3 with highest P(w3|w1,w2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUXcbTvZMN3q",
        "outputId": "ba19799c-e7eb-4b4a-96b9-2f95e7455c96"
      },
      "source": [
        "print (\"Generating sentence\")\n",
        "start_clock = time.time()\n",
        "generated_sent = generate_sentence_from_trigram_model(model, first_word_argmax = False, deterministic=True)\n",
        "end_clock = time.time()\n",
        "print (\"Generated sentence: %s\" %generated_sent)\n",
        "print (\"Time taken = %.3f seconds\" %((end_clock-start_clock)/60))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentence\n",
            "Generated sentence: interdisciplinary simulations between obstetrics and gynecology university of california san francisco\n",
            "Time taken = 0.001 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa0UikkrXk1Y"
      },
      "source": [
        "#### c) Complete Probabilistic sentence generation with each word selected randomly based on the probability distribution of P(w3|w1,w2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSPMfvCsXvu-",
        "outputId": "f0643ced-3a1c-4b87-d311-af0e8e471b4a"
      },
      "source": [
        "print (\"Generating sentence\")\n",
        "start_clock = time.time()\n",
        "generated_sent = generate_sentence_from_trigram_model(model, first_word_argmax = False, deterministic=False)\n",
        "end_clock = time.time()\n",
        "print (\"Generated sentence: %s\" %generated_sent)\n",
        "print (\"Time taken = %.3f seconds\" %((end_clock-start_clock)/60))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentence\n",
            "Generated sentence: figure shows the corresponding period in the biosensor\n",
            "Time taken = 0.001 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-bjn0LmMYya"
      },
      "source": [
        "Finding average probability of generating test sentences given trained three gram model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjOHTMcQkX1W"
      },
      "source": [
        "def find_probability_sentence_trigram(model, sn):\n",
        "  '''\n",
        "  function to find probability of generation of a test sentence given parameters of trigram language model\n",
        "  '''\n",
        "  epsilon = 1e-10\n",
        "  w1 = '<s>'\n",
        "  w2 = '<s>'\n",
        "\n",
        "  log_p_sum = 0\n",
        "  tokens = word_tokenize(sn)\n",
        "  words = [word for word in tokens if word.isalpha()]\n",
        "  \n",
        "  #appending end symbol to words\n",
        "  words.append('</s>')\n",
        "  words.append('</s>')\n",
        "  for word in words:\n",
        "    if word in model[(w1,w2)]: \n",
        "      p = model[(w1,w2)][word]\n",
        "      #print ((w1,w2,word), p)\n",
        "    else:\n",
        "      p = epsilon\n",
        "      #print ((w1, w2, word))\n",
        "    log_p_sum += np.log(p)    \n",
        "\n",
        "    w1 = w2\n",
        "    w2 = word\n",
        "\n",
        "  #print (len(words)+2, c)\n",
        "  return np.exp(log_p_sum)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbXt-MCTkduX"
      },
      "source": [
        "def evaluate_trigram_model_on_test(model, test_sentences):\n",
        "  #function to return the average probability of generating test corpus of sentences given the model parameters\n",
        "  avg_prob = 0\n",
        "  for sn in tqdm(test_sentences):\n",
        "    prob_sentence = find_probability_sentence_trigram(model, sn)\n",
        "    avg_prob += prob_sentence\n",
        "\n",
        "  avg_prob = avg_prob/(len(test_sentences))\n",
        "  return avg_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK7tGO6fslgb",
        "outputId": "b4b139fa-abb3-4920-abb6-b90afae38652"
      },
      "source": [
        "avg_prob = evaluate_trigram_model_on_test(model, test_sentences)\n",
        "print (\"\\nAverage probability of generating test corpus given three gram model parameters = %f\" %avg_prob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [00:22<00:00, 3118.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average probability of generating test corpus given three gram model parameters = 0.000026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oQ7jWs9wNaw"
      },
      "source": [
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTX6kv4GNXQc"
      },
      "source": [
        "## 4-gram model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz4cGeB7NWfs"
      },
      "source": [
        "def build_4_gram_model(sentences):\n",
        "  #function to return the four gram language model \n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  #iterating over sentences\n",
        "  for sentence in tqdm(sentences):\n",
        "    #tokenizing each sentence into words\n",
        "    tokens = word_tokenize(sentence)\n",
        "    words = [token for token in tokens if token.isalpha()]\n",
        "    four_grams = list(ngrams(words,4, pad_left = True, pad_right = True, left_pad_symbol = '<s>', right_pad_symbol = '</s>'))\n",
        "    #updating frequency of four gram tuple\n",
        "    for w1,w2,w3,w4 in four_grams:\n",
        "      model[(w1,w2,w3)][w4]+=1\n",
        "\n",
        "  #updating probability of four gram tuple\n",
        "  for w1_w2_w3 in model:\n",
        "    total_count = float(sum(model[w1_w2_w3].values()))\n",
        "    for w4 in model[w1_w2_w3]:\n",
        "        model[w1_w2_w3][w4] /= total_count\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkeF3zlNRofM"
      },
      "source": [
        "def generate_sentence_from_four_gram_model(model, first_word_argmax = False, deterministic = False):\n",
        "  '''function to generate sentence with four gram model given as input. The parameter \"deterministic\" is used to determine if the language to be generated is deterministic or not.\n",
        "  If the parameter \"first_word_argmax\" is True, then the first word generated is the word with the maximum probability. Else, the first word is also chosen based on model predicted word probability distribution.\n",
        "  For the rest of the words to be generated, if the parameter \"deterministic\" is True, then we generate word (w4) based on the \n",
        "  maximum probability given the previous two words(w1,w2,w3). If the parameter \"deterministic\" is False, then we generate a word randomly based on the probablity distribution of w4|w1,w2,w3.   \n",
        "  ''' \n",
        "  w1 = '<s>'\n",
        "  w2 = '<s>'\n",
        "  w3 = '<s>'\n",
        "  \n",
        "  sent = ''\n",
        "  words = list(model[(w1,w2,w3)].keys())\n",
        "  probs = list(model[(w1,w2,w3)].values())\n",
        "\n",
        "  if first_word_argmax == True:\n",
        "    word = words[np.argmax(probs)]\n",
        "  else:\n",
        "    word = np.random.choice(words, p = probs)\n",
        "    \n",
        "  first_word = word\n",
        "\n",
        "  sent = sent + first_word\n",
        "  sentence_finish = False\n",
        "  while not sentence_finish:\n",
        "    w1 = w2\n",
        "    w2 = w3\n",
        "    w3 = word\n",
        "\n",
        "    words = list(model[(w1,w2,w3)].keys())\n",
        "    probs = list(model[(w1,w2,w3)].values())\n",
        "\n",
        "    if not deterministic:\n",
        "      word = np.random.choice(words, p = probs)\n",
        "    else:\n",
        "      word = words[np.argmax(probs)]\n",
        "\n",
        "    #print (word)\n",
        "    if word=='</s>':\n",
        "      sentence_finish = True\n",
        "      break\n",
        "    sent = sent + ' ' + str(word)\n",
        "\n",
        "    \n",
        "  return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC7DY-ew23u-"
      },
      "source": [
        "**3) Build a language model using 4-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txr4KpMXBjze",
        "outputId": "47f44279-06e5-477b-f648-3fe2f49fc6bc"
      },
      "source": [
        "print (\"\\nBuilding 4-gram model\")\n",
        "start_clock = time.time()\n",
        "model = build_4_gram_model(train_sentences)\n",
        "end_clock = time.time()\n",
        "print (\"Model built\")\n",
        "\n",
        "print (\"Time taken = %.3f minutes\" %((end_clock-start_clock)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 390/630000 [00:00<02:41, 3896.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Building 4-gram model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 630000/630000 [03:12<00:00, 3271.64it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model built\n",
            "Time taken = 3.417 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEoNrte3CgM"
      },
      "source": [
        "**4) Generating sentence using 4-gram model**\n",
        "\n",
        "Just like for the sentence generation of the three gram model, I am also going to show how the model generates sentence based on three cases:\n",
        "\n",
        "#### a) First word is the word with highest occurence in the corpus and rest of the words generated are one with the highest P(w4|w1,w2,w3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km57zDM-Bw9J",
        "outputId": "868f2382-0b07-43c3-efc0-f8bd288f3ed9"
      },
      "source": [
        "print (\"Generating sentence\")\n",
        "start_clock = time.time()\n",
        "generated_sent = generate_sentence_from_four_gram_model(model, first_word_argmax = True, deterministic=True)\n",
        "end_clock = time.time()\n",
        "print (\"Generated sentence: %s\" %generated_sent)\n",
        "print (\"Time taken = %.3f minutes\" %((end_clock-start_clock)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentence\n",
            "Generated sentence: the copyright holder for this preprint this version posted may\n",
            "Time taken = 0.000 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qQJwK7xOSAz"
      },
      "source": [
        "#### b) First word is a word selected randomly based on model predicted P(first_word|start_token, start_token, start_token) and the rest of the words are chosen with highest P(w4|w1,w2,w3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KlXajY0QfK8",
        "outputId": "143409cc-9eff-4fd2-9e3e-b5300e0c0702"
      },
      "source": [
        "print (\"Generating sentence\")\n",
        "start_clock = time.time()\n",
        "generated_sent = generate_sentence_from_four_gram_model(model, first_word_argmax = False, deterministic=True)\n",
        "end_clock = time.time()\n",
        "print (\"Generated sentence: %s\" %generated_sent)\n",
        "print (\"Time taken = %.3f minutes\" %((end_clock-start_clock)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentence\n",
            "Generated sentence: these results suggest that the risk of infection\n",
            "Time taken = 0.001 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxJeobF4ZwBc"
      },
      "source": [
        "#### c) Complete probabilistic sentence where each word selected is chosen randomly based on model predicted P(w4|w1,w2,w3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1mdo5PmZ7bK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec0a9e6-aa76-4175-991c-694294dcc6fd"
      },
      "source": [
        "print (\"Generating sentence\")\n",
        "start_clock = time.time()\n",
        "generated_sent = generate_sentence_from_four_gram_model(model, first_word_argmax = False, deterministic=False)\n",
        "end_clock = time.time()\n",
        "print (\"Generated sentence: %s\" %generated_sent)\n",
        "print (\"Time taken = %.3f minutes\" %((end_clock-start_clock)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating sentence\n",
            "Generated sentence: mals is considered as the sum of viable eggs shed by infected humans\n",
            "Time taken = 0.002 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed51066AuWex"
      },
      "source": [
        "def find_probability_sentence_four_gram(model, sn):\n",
        "  '''\n",
        "  function to find probability of generation of a test sentence given parameters of 4 gram language model. If a 4-gram (w1,w2,w3,w4) is not present in the training set, we replace that probability\n",
        "  with epsilon which is a very small value and for our purpose we are choosing 10^-10.\n",
        "  \n",
        "  '''\n",
        "  epsilon = 1e-10\n",
        "  w1 = '<s>'\n",
        "  w2 = '<s>'\n",
        "  w3 = '<s>'\n",
        "\n",
        "  log_p_sum = 0\n",
        "  tokens = word_tokenize(sn)\n",
        "  words = [word for word in tokens if word.isalpha()]\n",
        "  \n",
        "  \n",
        "  words.append('</s>')\n",
        "  words.append('</s>')\n",
        "  words.append('</s>')\n",
        "\n",
        "  for word in words:\n",
        "    if word in model[(w1,w2,w3)]: \n",
        "      p = model[(w1,w2,w3)][word]\n",
        "      #print ((w1,w2,word), p)\n",
        "    else:\n",
        "      p = epsilon\n",
        "      #print ((w1, w2, word))\n",
        "    log_p_sum += np.log(p)\n",
        "        \n",
        "\n",
        "    w1 = w2\n",
        "    w2 = w3\n",
        "    w3 = word\n",
        "\n",
        "  #print (len(words)+2, c)\n",
        "  return np.exp(log_p_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMVAfVf1HOL9"
      },
      "source": [
        "def evaluate_four_gram_model_on_test(model, test_sentences):\n",
        "  #function to return the average probability of generating test corpus of sentences given the four gram model parameters\n",
        "  avg_prob = 0\n",
        "  for sn in tqdm(test_sentences):\n",
        "    prob_sentence = find_probability_sentence_four_gram(model, sn)\n",
        "    avg_prob += prob_sentence\n",
        "\n",
        "  avg_prob = avg_prob/(len(test_sentences))\n",
        "  return avg_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12q0HpNhRFxE",
        "outputId": "e1072fbe-abf4-4df0-c98f-2eea532bf90c"
      },
      "source": [
        "avg_prob = evaluate_four_gram_model_on_test(model, test_sentences)\n",
        "print (\"\\nAverage probability of generating test corpus given four gram model parameters = %f\" %avg_prob)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [00:22<00:00, 3179.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average probability of generating test corpus given four gram model parameters = 0.000028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7CDOITw3eyI"
      },
      "source": [
        "**5) The 4-gram model performed slightly better than the three gram langugage model. For each test sentence, I am finding the probability of the sentence based on the model parameters. Then I am averaging over the number of test sentences to find the average probability of test sentences based on model parameters. For 4-gram model, the average probability is slightly higher than the average probability of test sentences based on 3-gram model parameters.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5M437u35-Tr"
      },
      "source": [
        "**6) Efficient ways to handle the large set of parameters:**\n",
        "\n",
        "**a) One optimization that can be done to this is in handling the storage of tri-gram and 4-gram language models. Currently, I am storing the trigram model using a dictionary of dictionaries where the outer dictionary stores the tuple of words (w1, w2) as keys and the inner dictionary as value which is a dictionary with word w3 as key and P(w1,w2,w3) as value. Since, each character of a word takes one byte and the words can take any number of characters, the storage increases significantly. Here, we can have an integer mapping for the words and an inverse mapping from the integers to words as a separate map. While this takes an initial extra overhead in complexity, this will significantly improve storage complexity for the ngram models.**\n",
        "\n",
        "**b) Another optimization that can be done is while computing the frequency of tuples (w1,w2,w3). We can separately perform this operation on batches of train data parallely and then combine them together.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W0175TnfvGO"
      },
      "source": [
        "**7) Yes, it is possible to run some parts of the code in parallel. While building the ngram model, the first part involves computing the frequency of the tuple (w1, w2, w3,..,wn) for n grams w1,w2,...wn. Here, this frequency is done over the training sentences iteratively in a sequential manner. However, we can also perform this computation in batches in parallel and finally we can sum up the frequency values obtained for the same keys over batches to get the total frequency count. This would speed up building the model significantly.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRo1Iy1DpJuC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}